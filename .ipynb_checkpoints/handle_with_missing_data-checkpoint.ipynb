{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a2d8e3-75c2-4804-af5c-d197f3945462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in all our data\n",
    "nfl_data = pd.read_csv(\"../input/building-permit-applications-data/Building_Permits.csv\")\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db8f4ebf-5b7f-46b5-8b94-3a773f67d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of missing data points per column\n",
    "missing_values_count = nfl_data.isnull().sum()\n",
    "\n",
    "# look at the # of missing points in the first ten columns\n",
    "missing_values_count[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f891699-0ddf-446c-99a9-de9ecf3cb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many total missing values do we have?\n",
    "total_cells = np.product(nfl_data.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# percent of data that is missing\n",
    "percent_missing = (total_missing/total_cells) * 100\n",
    "print(percent_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c7435-64bd-4591-9510-a53c5bae07f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the # of missing points in the first ten columns\n",
    "missing_values_count[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd047f-8b4e-4dc6-9db5-902c05fa331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the rows that contain a missing value\n",
    "nfl_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a01536-b361-4894-983d-7f90651c077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all columns with at least one missing value\n",
    "columns_with_na_dropped = nfl_data.dropna(axis=1)\n",
    "columns_with_na_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7ff55-4680-403c-b1f3-21286ef3b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just how much data did we lose?\n",
    "print(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])\n",
    "print(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23fb8e-9ccb-4496-82c1-6930982f14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a small subset of the NFL dataset\n",
    "subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()\n",
    "subset_nfl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267202a0-d1ce-4a2d-8132-bbcc4b26a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all NA's with 0\n",
    "subset_nfl_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50bf7d-958d-42fe-8bc8-507e5df336f5",
   "metadata": {},
   "source": [
    "## Scaling and Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c190879-4650-4abb-ad4c-77b9181a6eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for Box-Cox Transformation\n",
    "from scipy import stats\n",
    "\n",
    "# for min_max scaling\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "# plotting modules\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb754c-1894-4bd8-8ff2-43040f731650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1000 data points randomly drawn from an exponential distribution\n",
    "original_data = np.random.exponential(size=1000)\n",
    "\n",
    "# mix-max scale the data between 0 and 1\n",
    "scaled_data = minmax_scaling(original_data, columns=[0])\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax = plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(scaled_data, ax=ax[1])\n",
    "ax[1].set_title(\"Scaled data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a012d2-1258-44a8-a87b-76638a2817df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the exponential data with boxcox\n",
    "normalized_data = stats.boxcox(original_data)\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(normalized_data[0], ax=ax[1])\n",
    "ax[1].set_title(\"Normalized data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5823c21b-4f16-47f2-a0fc-7f75aa393bed",
   "metadata": {},
   "source": [
    "## Parsing Dates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d3d67-9cdc-4394-a055-aabef84e62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "# read in our data\n",
    "landslides = pd.read_csv(\"../input/landslide-events/catalog.csv\")\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b35f4-d75e-48b3-b431-948be31b3b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "landslides.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7942c70d-7a94-47ba-a432-42fa83d6ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first few rows of the date column\n",
    "print(landslides['date'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d199f9e7-1717-4082-b3d8-73ba061b3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data type of our date column\n",
    "landslides['date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd63e27c-1861-45a4-a882-2f3e2d5ba64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column, date_parsed, with the parsed dates\n",
    "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format=\"%m/%d/%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4051b-8819-4e79-a884-c2c0f7d4b08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first few rows\n",
    "landslides['date_parsed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da571961-aaf2-4ca5-a35c-f9448257d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the day of the month from the date_parsed column\n",
    "day_of_month_landslides = landslides['date_parsed'].dt.day\n",
    "day_of_month_landslides.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f449e71-b7c4-4312-ac78-c8e81c318d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove na's\n",
    "day_of_month_landslides = day_of_month_landslides.dropna()\n",
    "\n",
    "# plot the day of the month\n",
    "sns.distplot(day_of_month_landslides, kde=False, bins=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a26981-924f-491f-bbba-cfbd31cf835b",
   "metadata": {},
   "source": [
    "## Character Encodings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f9a839-a6ff-43f0-97d7-6818bf92a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# helpful character encoding module\n",
    "import chardet\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e15f64-5583-414e-9a7e-fd65a28c6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a string\n",
    "before = \"This is the euro symbol: €\"\n",
    "\n",
    "# check to see what datatype it is\n",
    "type(before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb785a7-08e9-4172-bd26-31847d3f3c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode it to a different encoding, replacing characters that raise errors\n",
    "after = before.encode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "# check the type\n",
    "type(after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f040d6-af38-49f6-b671-87ced753a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at what the bytes look like\n",
    "after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414097c-cc87-4b86-b430-904c81329f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it back to utf-8\n",
    "print(after.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab1569-fd81-4081-8cb1-05a876896b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to decode our bytes with the ascii encoding\n",
    "print(after.decode(\"ascii\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cfe1d8-0d84-4127-bb0d-87b62858dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with a string\n",
    "before = \"This is the euro symbol: €\"\n",
    "\n",
    "# encode it to a different encoding, replacing characters that raise errors\n",
    "after = before.encode(\"ascii\", errors = \"replace\")\n",
    "\n",
    "# convert it back to utf-8\n",
    "print(after.decode(\"ascii\"))\n",
    "\n",
    "# We've lost the original underlying byte string! It's been \n",
    "# replaced with the underlying byte string for the unknown character :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c8616a-8c99-4566-9355-9ad511629c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to read in a file not in UTF-8\n",
    "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b963c-f022-4913-840d-5f2fecd41e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first ten thousand bytes to guess the character encoding\n",
    "with open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(10000))\n",
    "\n",
    "# check what the character encoding might be\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb0ecf-48ca-41c2-ac50-4d70a0338776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file with the encoding detected by chardet\n",
    "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')\n",
    "\n",
    "# look at the first few lines\n",
    "kickstarter_2016.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f73a07-6c53-41ae-9030-a19e00bd2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our file (will be saved as UTF-8 by default!)\n",
    "kickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20dac8d-ad75-4117-98e7-fd049617a5cd",
   "metadata": {},
   "source": [
    "## Inconsistent Data Entry\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa47142-bf0e-4664-a485-373e944ebf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# helpful modules\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "import chardet\n",
    "\n",
    "# read in all our data\n",
    "professors = pd.read_csv(\"../input/pakistan-intellectual-capital/pakistan_intellectual_capital.csv\")\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667dfde-e03f-467a-bd1f-ca4fee8fdaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "professors.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf3e9b-8834-435c-9569-c98db3a339b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique values in the 'Country' column\n",
    "countries = professors['Country'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "countries.sort()\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5f312-29f7-4d47-a09f-6bf7136f5d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "professors['Country'] = professors['Country'].str.lower()\n",
    "# remove trailing white spaces\n",
    "professors['Country'] = professors['Country'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61947332-165e-4233-96f1-cc6980e16cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique values in the 'Country' column\n",
    "countries = professors['Country'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "countries.sort()\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0056c0-eee7-4932-ad03-b676c46560aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 10 closest matches to \"south korea\"\n",
    "matches = fuzzywuzzy.process.extract(\"south korea\", countries, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "# take a look at them\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d967a8-198b-4baa-8b94-d3d3417a19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace rows in the provided column of the provided dataframe\n",
    "# that match the provided string above the provided ratio with the provided string\n",
    "def replace_matches_in_column(df, column, string_to_match, min_ratio = 47):\n",
    "    # get a list of unique strings\n",
    "    strings = df[column].unique()\n",
    "    \n",
    "    # get the top 10 closest matches to our input string\n",
    "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
    "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "\n",
    "    # only get matches with a ratio > 90\n",
    "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
    "\n",
    "    # get the rows of all the close matches in our dataframe\n",
    "    rows_with_matches = df[column].isin(close_matches)\n",
    "\n",
    "    # replace all rows with close matches with the input matches \n",
    "    df.loc[rows_with_matches, column] = string_to_match\n",
    "    \n",
    "    # let us know the function's done\n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333322d-d680-4570-8f71-3acad31c88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function we just wrote to replace close matches to \"south korea\" with \"south korea\"\n",
    "replace_matches_in_column(df=professors, column='Country', string_to_match=\"south korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7b9dc6-c467-4505-bb8a-3f5ed2760dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique values in the 'Country' column\n",
    "countries = professors['Country'].unique()\n",
    "\n",
    "# sort them alphabetically and then take a closer look\n",
    "countries.sort()\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9451249-c172-46f7-ad3b-b2323e3148a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d3f80-09a2-4b1b-a5eb-1db5ae9410cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c48512-742d-49d8-8001-1240b8ca6395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
