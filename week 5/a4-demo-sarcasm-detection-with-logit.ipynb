{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n<img src=\"https://habrastorage.org/files/fd4/502/43d/fd450243dd604b81b9713213a247aa20.jpg\">\n    \n## [mlcourse.ai](https://mlcourse.ai) â€“ Open Machine Learning Course \nAuthor: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose.","metadata":{"_uuid":"3f6c2bfe6b2e26c92357e896a1511195d836956e"}},{"cell_type":"markdown","source":"## <center> Assignment 4. Sarcasm detection with logistic regression\n    \nWe'll be using the dataset from the [paper](https://arxiv.org/abs/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https://www.kaggle.com/danofer/sarcasm).\n\nSarcasm detection is easy. \n<img src=\"https://habrastorage.org/webt/1f/0d/ta/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" />","metadata":{"_uuid":"cb01ca96934e5c83a36a2308da9645b87a9c52a0"}},{"cell_type":"code","source":"!ls ../input/sarcasm/","metadata":{"_uuid":"23a833b42b3c214b5191dfdc2482f2f901118247","execution":{"iopub.status.busy":"2021-11-24T08:06:16.128403Z","iopub.execute_input":"2021-11-24T08:06:16.129012Z","iopub.status.idle":"2021-11-24T08:06:16.894157Z","shell.execute_reply.started":"2021-11-24T08:06:16.128964Z","shell.execute_reply":"2021-11-24T08:06:16.893111Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"ffa03aec57ab6150f9bec0fa56cd3a5791a3e6f4","execution":{"iopub.status.busy":"2021-11-24T08:06:16.896194Z","iopub.execute_input":"2021-11-24T08:06:16.896483Z","iopub.status.idle":"2021-11-24T08:06:18.122837Z","shell.execute_reply.started":"2021-11-24T08:06:16.896433Z","shell.execute_reply":"2021-11-24T08:06:18.121741Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/sarcasm/train-balanced-sarcasm.csv')","metadata":{"_uuid":"b23e4fc7a1973d60e0c6da8bd60f3d921542a856","execution":{"iopub.status.busy":"2021-11-24T08:06:18.124118Z","iopub.execute_input":"2021-11-24T08:06:18.124362Z","iopub.status.idle":"2021-11-24T08:06:24.485475Z","shell.execute_reply.started":"2021-11-24T08:06:18.124326Z","shell.execute_reply":"2021-11-24T08:06:24.484479Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"_uuid":"4dc7b3787afa46c7eb0d0e33b0c41ab9821c4a27","execution":{"iopub.status.busy":"2021-11-24T08:06:24.486642Z","iopub.execute_input":"2021-11-24T08:06:24.486900Z","iopub.status.idle":"2021-11-24T08:06:24.527933Z","shell.execute_reply.started":"2021-11-24T08:06:24.486855Z","shell.execute_reply":"2021-11-24T08:06:24.527288Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"_uuid":"0a7ed9557943806c6813ad59c3d5ebdb403ffd78","execution":{"iopub.status.busy":"2021-11-24T08:06:24.529183Z","iopub.execute_input":"2021-11-24T08:06:24.529700Z","iopub.status.idle":"2021-11-24T08:06:25.356262Z","shell.execute_reply.started":"2021-11-24T08:06:24.529640Z","shell.execute_reply":"2021-11-24T08:06:25.355537Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Some comments are missing, so we drop the corresponding rows.","metadata":{"_uuid":"6472f52fb5ecb8bb2a6e3b292678a2042fcfe34c"}},{"cell_type":"code","source":"train_df.dropna(subset=['comment'], inplace=True)","metadata":{"_uuid":"97b2d85627fcde52a506dbdd55d4d6e4c87d3f08","execution":{"iopub.status.busy":"2021-11-24T08:06:25.357248Z","iopub.execute_input":"2021-11-24T08:06:25.357601Z","iopub.status.idle":"2021-11-24T08:06:25.677131Z","shell.execute_reply.started":"2021-11-24T08:06:25.357562Z","shell.execute_reply":"2021-11-24T08:06:25.676142Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"We notice that the dataset is indeed balanced","metadata":{"_uuid":"9d51637ee70dca7693737ad0da1dbb8c6ce9230b"}},{"cell_type":"code","source":"train_df['label'].value_counts()","metadata":{"_uuid":"addd77c640423d30fd146c8d3a012d3c14481e11","execution":{"iopub.status.busy":"2021-11-24T08:06:25.678845Z","iopub.execute_input":"2021-11-24T08:06:25.679277Z","iopub.status.idle":"2021-11-24T08:06:25.700776Z","shell.execute_reply.started":"2021-11-24T08:06:25.679188Z","shell.execute_reply":"2021-11-24T08:06:25.699865Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We split data into training and validation parts.","metadata":{"_uuid":"5b836574e5093c5eb2e9063fefe1c8d198dcba79"}},{"cell_type":"code","source":"train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17)","metadata":{"_uuid":"c200add4e1dcbaa75164bbcc73b9c12ecb863c96","execution":{"iopub.status.busy":"2021-11-24T08:06:25.702029Z","iopub.execute_input":"2021-11-24T08:06:25.702495Z","iopub.status.idle":"2021-11-24T08:06:25.987406Z","shell.execute_reply.started":"2021-11-24T08:06:25.702427Z","shell.execute_reply":"2021-11-24T08:06:25.986632Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Tasks:\n1. Analyze the dataset, make some plots. This [Kernel](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc) might serve as an example\n2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n3. Plot the words/bigrams which a most predictive of sarcasm (you can use [eli5](https://github.com/TeamHG-Memex/eli5) for that)\n4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n\n## Links:\n  - Machine learning library [Scikit-learn](https://scikit-learn.org/stable/index.html) (a.k.a. sklearn)\n  - Kernels on [logistic regression](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-2-classification) and its applications to [text classification](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n  - [Kaggle Kernel](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n  - [ELI5](https://github.com/TeamHG-Memex/eli5) to explain model predictions","metadata":{"_uuid":"7f0f47b98e49a185cd5cffe19fcbe28409bf00c0"}},{"cell_type":"code","source":"sub_df = train_df.groupby('subreddit')['label'].agg([np.size, np.mean, np.sum])\nsub_df.sort_values(by='sum', ascending=False).head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T08:06:25.988601Z","iopub.execute_input":"2021-11-24T08:06:25.989065Z","iopub.status.idle":"2021-11-24T08:06:26.374417Z","shell.execute_reply.started":"2021-11-24T08:06:25.989007Z","shell.execute_reply":"2021-11-24T08:06:26.373490Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"sub_df[sub_df['size'] > 1000].sort_values(by='mean', ascending=False).head(10)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-24T08:06:26.375843Z","iopub.execute_input":"2021-11-24T08:06:26.376385Z","iopub.status.idle":"2021-11-24T08:06:26.395701Z","shell.execute_reply.started":"2021-11-24T08:06:26.376322Z","shell.execute_reply":"2021-11-24T08:06:26.394703Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline \n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-11-24T08:06:26.397558Z","iopub.execute_input":"2021-11-24T08:06:26.398131Z","iopub.status.idle":"2021-11-24T08:06:26.409125Z","shell.execute_reply.started":"2021-11-24T08:06:26.397906Z","shell.execute_reply":"2021-11-24T08:06:26.408358Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# build bigrams, put a limit on maximal number of features\n# and minimal word frequency\ntf_idf = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)\n# multinomial logistic regression a.k.a softmax classifier\nlogit = LogisticRegression(C=1, n_jobs=4, solver='lbfgs', \n                           random_state=17, verbose=1)\n# sklearn's pipeline\ntfidf_logit_pipeline = Pipeline([('tf_idf', tf_idf), \n                                 ('logit', logit)])","metadata":{"execution":{"iopub.status.busy":"2021-11-24T08:07:35.072710Z","iopub.execute_input":"2021-11-24T08:07:35.073430Z","iopub.status.idle":"2021-11-24T08:07:35.078492Z","shell.execute_reply.started":"2021-11-24T08:07:35.073372Z","shell.execute_reply":"2021-11-24T08:07:35.077826Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"%%time\ntfidf_logit_pipeline.fit(train_texts, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T08:07:50.510774Z","iopub.execute_input":"2021-11-24T08:07:50.511160Z","iopub.status.idle":"2021-11-24T08:09:01.376848Z","shell.execute_reply.started":"2021-11-24T08:07:50.511103Z","shell.execute_reply":"2021-11-24T08:09:01.375734Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"%%time\nvalid_pred = tfidf_logit_pipeline.predict(valid_texts)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T08:09:01.380268Z","iopub.execute_input":"2021-11-24T08:09:01.380583Z","iopub.status.idle":"2021-11-24T08:09:10.597159Z","shell.execute_reply.started":"2021-11-24T08:09:01.380520Z","shell.execute_reply":"2021-11-24T08:09:10.596123Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_valid, valid_pred)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-24T08:09:10.598939Z","iopub.execute_input":"2021-11-24T08:09:10.599574Z","iopub.status.idle":"2021-11-24T08:09:10.635134Z","shell.execute_reply.started":"2021-11-24T08:09:10.599431Z","shell.execute_reply":"2021-11-24T08:09:10.634148Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(actual, predicted, classes,\n                          normalize=False,\n                          title='Confusion matrix', figsize=(7,7),\n                          cmap=plt.cm.Blues, path_to_save_fig=None):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    from sklearn.metrics import confusion_matrix\n    cm = confusion_matrix(actual, predicted).T\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=figsize)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Predicted label')\n    plt.xlabel('True label')\n    \n    if path_to_save_fig:\n        plt.savefig(path_to_save_fig, dpi=300, bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2021-11-24T08:09:10.637168Z","iopub.execute_input":"2021-11-24T08:09:10.637565Z","iopub.status.idle":"2021-11-24T08:09:10.649134Z","shell.execute_reply.started":"2021-11-24T08:09:10.637489Z","shell.execute_reply":"2021-11-24T08:09:10.647753Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(y_valid, valid_pred, \n                      tfidf_logit_pipeline.named_steps['logit'].classes_, figsize=(8, 8))","metadata":{"execution":{"iopub.status.busy":"2021-11-24T08:09:10.650702Z","iopub.execute_input":"2021-11-24T08:09:10.651084Z","iopub.status.idle":"2021-11-24T08:09:11.439697Z","shell.execute_reply.started":"2021-11-24T08:09:10.651016Z","shell.execute_reply":"2021-11-24T08:09:11.438626Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import eli5\neli5.show_weights(estimator=tfidf_logit_pipeline.named_steps['logit'],\n                  vec=tfidf_logit_pipeline.named_steps['tf_idf'])","metadata":{"execution":{"iopub.status.busy":"2021-11-24T08:09:11.442942Z","iopub.execute_input":"2021-11-24T08:09:11.443529Z","iopub.status.idle":"2021-11-24T08:09:12.014747Z","shell.execute_reply.started":"2021-11-24T08:09:11.443461Z","shell.execute_reply":"2021-11-24T08:09:12.013587Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}